import streamlit as st
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from PIL import Image
import cv2
import numpy as np
import os
import time
import logging
from pydub import AudioSegment
import ffmpeg
import requests
from tqdm import tqdm

# é…ç½®æ—¥å¿—
logging.basicConfig(filename="logs/error.log", level=logging.ERROR, format="%(asctime)s - %(levelname)s - %(message)s")

# éœ“è™¹é£æ ¼ CSS
st.markdown("""
<style>
body { background-color: #1a1a2e; color: #e0e0e0; font-family: 'Arial', sans-serif; }
.stApp { background: linear-gradient(135deg, #ff007f, #00ddeb); }
.stButton>button { background-color: #ff007f; color: white; border: none; padding: 10px 20px; border-radius: 5px; }
.stButton>button:hover { background-color: #00ddeb; }
.stFileUploader, .stSelectbox { background-color: #16213e; border-radius: 5px; padding: 10px; }
.stSpinner { color: #00ddeb; }
.stTabs { background-color: #0f3460; border-radius: 5px; }
.stTabs > div > button { color: #e0e0e0; }
.stTabs > div > button:hover { background-color: #ff007f; }
.stCanvas { border: 2px solid #00ddeb; }
</style>
""", unsafe_allow_html=True)

# GPU ä¿¡æ¯
def get_gpu_info():
    try:
        torch.cuda.init()
        return f"GPU: {torch.cuda.get_device_name(0)}, å¯ç”¨å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**2:.2f} MB"
    except:
        return "æ— æ³•è·å– GPU ä¿¡æ¯"

# DCGAN æ¨¡å‹ï¼ˆå›¾åƒç”Ÿæˆï¼‰
class Generator(nn.Module):
    def __init__(self, latent_dim=100):
        super(Generator, self).__init__()
        self.model = nn.Sequential(
            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(True),
            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.ReLU(True),
            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(True),
            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(True),
            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),
            nn.Tanh()
        )

    def forward(self, x):
        return self.model(x)

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 64, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(64, 128, 4, 2, 1, bias=False),
            nn.BatchNorm2d(128),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(128, 256, 4, 2, 1, bias=False),
            nn.BatchNorm2d(256),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(256, 512, 4, 2, 1, bias=False),
            nn.BatchNorm2d(512),
            nn.LeakyReLU(0.2, inplace=True),
            nn.Conv2d(512, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# U-Net æ¨¡å‹ï¼ˆä¿®å¤/å¢å¼ºï¼‰
class UNet(nn.Module):
    def __init__(self):
        super(UNet, self).__init__()
        self.enc1 = nn.Conv2d(3, 64, 3, padding=1)
        self.enc2 = nn.Conv2d(64, 128, 3, padding=1)
        self.enc3 = nn.Conv2d(128, 256, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dec1 = nn.ConvTranspose2d(256, 128, 2, stride=2)
        self.dec2 = nn.ConvTranspose2d(256, 64, 2, stride=2)
        self.dec3 = nn.Conv2d(128, 3, 3, padding=1)

    def forward(self, x):
        e1 = torch.relu(self.enc1(x))
        e2 = torch.relu(self.enc2(self.pool(e1)))
        e3 = torch.relu(self.enc3(self.pool(e2)))
        d1 = torch.relu(self.dec1(e3))
        d2 = torch.relu(self.dec2(torch.cat([d1, e2], dim=1)))
        d3 = torch.tanh(self.dec3(torch.cat([d2, e1], dim=1)))
        return d3

# è®­ç»ƒ DCGAN
def train_dcgan(generator, discriminator, data_loader, epochs=10):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    generator.to(device)
    discriminator.to(device)
    criterion = nn.BCELoss()
    g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))
    
    os.makedirs("weights", exist_ok=True)
    for epoch in range(epochs):
        for i, (images, _) in enumerate(tqdm(data_loader, desc=f"Epoch {epoch+1}")):
            batch_size = images.size(0)
            images = images.to(device)
            real_labels = torch.ones(batch_size, 1).to(device)
            fake_labels = torch.zeros(batch_size, 1).to(device)
            d_optimizer.zero_grad()
            real_output = discriminator(images)
            d_real_loss = criterion(real_output, real_labels)
            z = torch.randn(batch_size, 100, 1, 1).to(device)
            fake_images = generator(z)
            fake_output = discriminator(fake_images.detach())
            d_fake_loss = criterion(fake_output, fake_labels)
            d_loss = d_real_loss + d_fake_loss
            d_loss.backward()
            d_optimizer.step()
            g_optimizer.zero_grad()
            fake_output = discriminator(fake_images)
            g_loss = criterion(fake_output, real_labels)
            g_loss.backward()
            g_optimizer.step()
        torch.save(generator.state_dict(), f"weights/generator_epoch_{epoch+1}.pth")
        torch.save(discriminator.state_dict(), f"weights/discriminator_epoch_{epoch+1}.pth")
        st.write(f"Epoch {epoch+1}, D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}")

# è®­ç»ƒ U-Net
def train_unet(model, data_loader, epochs=10):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(epochs):
        for i, (images, _) in enumerate(tqdm(data_loader, desc=f"Epoch {epoch+1}")):
            images = images.to(device)
            optimizer.zero_grad()
            output = model(images)
            loss = criterion(output, images)
            loss.backward()
            optimizer.step()
        torch.save(model.state_dict(), f"weights/unet_epoch_{epoch+1}.pth")
        st.write(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

# ä¸‹è½½æ•°æ®é›†
def download_dataset():
    os.makedirs("data", exist_ok=True)
    if not os.path.exists("data/coco"):
        st.write("ä¸‹è½½ COCO æ•°æ®é›†...")
        # ç¤ºä¾‹ï¼šå®é™…éœ€æ›¿æ¢ä¸ºçœŸå®ä¸‹è½½é€»è¾‘
        os.makedirs("data/coco")
    if not os.path.exists("data/ucf101"):
        st.write("ä¸‹è½½ UCF101 æ•°æ®é›†...")
        os.makedirs("data/ucf101")

# å›¾åƒç”Ÿæˆ
def generate_image(generator, prompt, resolution):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    generator.eval()
    z = torch.randn(1, 100, 1, 1).to(device)
    with torch.no_grad():
        image = generator(z)
    image = (image + 1) / 2 * 255
    image = image.squeeze(0).permute(1, 2, 0).cpu().numpy().astype(np.uint8)
    if resolution == "1024x1024":
        image = cv2.resize(image, (1024, 1024))
    return image

# å›¾åƒä¿®å¤
def inpaint_image(unet, image, mask):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    unet.eval()
    image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
    mask = torch.from_numpy(mask).float() / 255.0
    image = image.unsqueeze(0).to(device)
    mask = mask.unsqueeze(0).unsqueeze(0).to(device)
    with torch.no_grad():
        output = unet(image * (1 - mask))
    output = output.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
    return output.astype(np.uint8)

# å›¾åƒå¢å¼º
def enhance_image(unet, image, scale):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    unet.eval()
    image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0
    image = image.unsqueeze(0).to(device)
    with torch.no_grad():
        output = unet(image)
    output = output.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255
    if scale == 4:
        output = cv2.resize(output, (image.shape[2] * 4, image.shape[1] * 4))
    return output.astype(np.uint8)

# è§†é¢‘ç”Ÿæˆ
def generate_video(generator, prompt, duration, fps, resolution):
    frames = []
    num_frames = int(duration * fps)
    for _ in range(num_frames):
        frame = generate_image(generator, prompt, resolution)
        frames.append(frame)
    return frames

# è¡¥å¸§
def interpolate_frames(frames, target_fps):
    interpolated = []
    for i in range(len(frames) - 1):
        interpolated.append(frames[i])
        mid_frame = (frames[i].astype(float) + frames[i + 1].astype(float)) / 2
        interpolated.append(mid_frame.astype(np.uint8))
    interpolated.append(frames[-1])
    return interpolated

# éŸ³é¢‘ç”Ÿæˆï¼ˆä¼˜åŒ–ï¼Œä½¿ç”¨ pydubï¼‰
def generate_audio(audio_type, file=None, operation=None, value=None):
    try:
        if audio_type == "èƒŒæ™¯éŸ³æ•ˆ":
            # ç”Ÿæˆç®€å•éŸ³æ•ˆï¼ˆç¤ºä¾‹ï¼šç™½å™ªéŸ³ï¼‰
            audio = AudioSegment.silent(duration=1000)
            result_path = f"cache/audio_{int(time.time())}.mp3"
            audio.export(result_path, format="mp3")
        else:
            audio = AudioSegment.from_file(file.name)
            if operation == "éŸ³é‡è°ƒæ•´":
                audio = audio + value
            elif operation == "è£å‰ª":
                audio = audio[:value * 1000]
            result_path = f"cache/audio_{int(time.time())}_{file.name}"
            audio.export(result_path, format=file.name.split(".")[-1])
        return result_path
    except Exception as e:
        logging.error(f"éŸ³é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

# ä¸»å‡½æ•°
def main():
    st.title("AI Toolkit ğŸ¨âœ¨")
    st.write(get_gpu_info())
    
    # ä¸‹è½½æ•°æ®é›†
    download_dataset()
    
    # åŠ è½½æˆ–è®­ç»ƒæ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    generator = Generator().to(device)
    discriminator = Discriminator().to(device)
    unet = UNet().to(device)
    
    # ç¤ºä¾‹æ•°æ®é›†åŠ è½½
    transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    dataset = torchvision.datasets.CIFAR10(root="data", train=True, download=True, transform=transform)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)
    
    # è®­ç»ƒé€‰é¡¹
    if st.button("è®­ç»ƒ DCGAN"):
        train_dcgan(generator, discriminator, data_loader)
    if st.button("è®­ç»ƒ U-Net"):
        train_unet(unet, data_loader)
    
    tabs = st.tabs(["å›¾åƒç”Ÿæˆ", "å›¾åƒå¤„ç†", "è§†é¢‘ç”Ÿæˆ", "è§†é¢‘å¤„ç†", "éŸ³é¢‘ç”Ÿæˆ"])
    
    # å›¾åƒç”Ÿæˆ
    with tabs[0]:
        st.header("å›¾åƒç”Ÿæˆ")
        prompt = st.text_input("è¾“å…¥ç”Ÿæˆæç¤º", "A futuristic city at night with neon lights")
        resolution = st.selectbox("åˆ†è¾¨ç‡", ["512x512", "1024x1024"])
        style = st.selectbox("é£æ ¼", ["ç°å®", "è‰ºæœ¯", "éœ“è™¹"])
        if st.button("ç”Ÿæˆå›¾åƒ", key="gen_image"):
            with st.spinner("ç”Ÿæˆä¸­..."):
                image = generate_image(generator, prompt, resolution)
                result_path = f"cache/image_{int(time.time())}.png"
                cv2.imwrite(result_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
                st.image(result_path, caption="ç”Ÿæˆå›¾åƒ")
                st.download_button("ä¸‹è½½", open(result_path, "rb").read(), "generated_image.png")
    
    # å›¾åƒå¤„ç†
    with tabs[1]:
        st.header("å›¾åƒå¤„ç†")
        col1, col2 = st.columns([0.3, 0.7])
        with col1:
            image = st.file_uploader("ä¸Šä¼ å›¾åƒ", ["png", "jpeg"], key="image_process")
            process_type = st.selectbox("å¤„ç†ç±»å‹", ["ä¿®å¤", "å¢å¼º", "é£æ ¼è½¬æ¢"])
            if process_type == "ä¿®å¤":
                st.write("ç»˜åˆ¶é®ç½©")
                canvas = st_canvas(fill_color="black", stroke_color="white", background_image=Image.open(image) if image else None, height=400, width=600, drawing_mode="freedraw", key="inpaint_canvas")
            elif process_type == "å¢å¼º":
                scale = st.selectbox("æ”¾å¤§å€æ•°", [2, 4])
        with col2:
            if st.button("å¤„ç†å›¾åƒ", key="process_image"):
                with st.spinner("å¤„ç†ä¸­..."):
                    img = cv2.imread(image.name)
                    if process_type == "ä¿®å¤":
                        mask = cv2.cvtColor(np.array(canvas.image_data), cv2.COLOR_RGBA2GRAY)
                        result = inpaint_image(unet, img, mask)
                    elif process_type == "å¢å¼º":
                        result = enhance_image(unet, img, scale)
                    else:
                        result = img  # é£æ ¼è½¬æ¢å¾…å®ç°
                    result_path = f"cache/process_{int(time.time())}_{image.name}"
                    cv2.imwrite(result_path, cv2.cvtColor(result, cv2.COLOR_RGB2BGR))
                    st.image(result_path, caption="å¤„ç†ç»“æœ")
                    st.download_button("ä¸‹è½½", open(result_path, "rb").read(), image.name)
    
    # è§†é¢‘ç”Ÿæˆ
    with tabs[2]:
        st.header("è§†é¢‘ç”Ÿæˆ")
        video_type = st.selectbox("ç”Ÿæˆç±»å‹", ["æ–‡æœ¬åˆ°è§†é¢‘", "å›¾åƒåˆ°è§†é¢‘"])
        if video_type == "æ–‡æœ¬åˆ°è§†é¢‘":
            prompt = st.text_input("è¾“å…¥ç”Ÿæˆæç¤º", "A car driving in the rain")
        else:
            image = st.file_uploader("ä¸Šä¼ åˆå§‹å›¾åƒ", ["png", "jpeg"], key="video_image")
        duration = st.slider("æ—¶é•¿ï¼ˆç§’ï¼‰", 5, 60, 10)
        fps = st.selectbox("å¸§ç‡", [24, 30, 60])
        resolution = st.selectbox("ç”»è´¨", ["720p", "1080p", "4K"])
        if st.button("ç”Ÿæˆè§†é¢‘", key="gen_video"):
            with st.spinner("ç”Ÿæˆä¸­..."):
                frames = generate_video(generator, prompt, duration, fps, resolution)
                result_path = f"cache/video_{int(time.time())}.mp4"
                out = cv2.VideoWriter(result_path, cv2.VideoWriter_fourcc(*"mp4v"), fps, (frames[0].shape[1], frames[0].shape[0]))
                for frame in frames:
                    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
                out.release()
                st.video(result_path)
                st.download_button("ä¸‹è½½", open(result_path, "rb").read(), "generated_video.mp4")
    
    # è§†é¢‘å¤„ç†
    with tabs[3]:
        st.header("è§†é¢‘å¤„ç†")
        video = st.file_uploader("ä¸Šä¼ è§†é¢‘", ["mp4"], key="video_process")
        process_type = st.selectbox("å¤„ç†ç±»å‹", ["è¡¥å¸§", "å¢å¼º"])
        if process_type == "è¡¥å¸§":
            target_fps = st.selectbox("ç›®æ ‡å¸§ç‡", [24, 30, 60])
        else:
            scale = st.selectbox("æ”¾å¤§å€æ•°", [2, 4])
        if st.button("å¤„ç†è§†é¢‘", key="process_video"):
            with st.spinner("å¤„ç†ä¸­..."):
                cap = cv2.VideoCapture(video.name)
                frames = []
                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break
                    frames.append(frame)
                cap.release()
                if process_type == "è¡¥å¸§":
                    frames = interpolate_frames(frames, target_fps)
                else:
                    frames = [enhance_image(unet, frame, scale) for frame in frames]
                result_path = f"cache/process_{int(time.time())}_{video.name}"
                out = cv2.VideoWriter(result_path, cv2.VideoWriter_fourcc(*"mp4v"), target_fps if process_type == "è¡¥å¸§" else 30, (frames[0].shape[1], frames[0].shape[0]))
                for frame in frames:
                    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
                out.release()
                st.video(result_path)
                st.download_button("ä¸‹è½½", open(result_path, "rb").read(), video.name)
    
    # éŸ³é¢‘ç”Ÿæˆï¼ˆä¼˜åŒ–ï¼‰
    with tabs[4]:
        st.header("éŸ³é¢‘ç”Ÿæˆ")
        audio_type = st.selectbox("ç”Ÿæˆç±»å‹", ["èƒŒæ™¯éŸ³æ•ˆ", "ä¸Šä¼ éŸ³é¢‘å¤„ç†"])
        if audio_type == "ä¸Šä¼ éŸ³é¢‘å¤„ç†":
            audio = st.file_uploader("ä¸Šä¼ éŸ³é¢‘", ["mp3", "wav"], key="audio_file")
            operation = st.selectbox("æ“ä½œ", ["éŸ³é‡è°ƒæ•´", "è£å‰ª"])
            value = st.slider("å€¼", -20, 20, 0) if operation == "éŸ³é‡è°ƒæ•´" else st.number_input("è£å‰ªæ—¶é•¿ï¼ˆç§’ï¼‰", 1, 300, 10)
        if st.button("ç”ŸæˆéŸ³é¢‘", key="gen_audio"):
            with st.spinner("å¤„ç†ä¸­..."):
                result = generate_audio(audio_type, audio, operation, value)
                if result:
                    st.audio(result)
                    st.download_button("ä¸‹è½½", open(result, "rb").read(), "generated_audio.mp3")
                else:
                    st.error("éŸ³é¢‘ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥ logs/error.log")

if __name__ == "__main__":
    main()
